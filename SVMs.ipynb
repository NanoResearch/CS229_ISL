{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "SVMs - "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Lance Martin"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quick cheatsheet - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Algorithm type} -$\n",
      "\n",
      "* Classification: It will compute a class label.\n",
      "* Discriminatative.\n",
      "\n",
      "$\\textbf{Model} -$\n",
      "\n",
      "* Classes for $y \\in \\{-1,1\\}$.\n",
      "* $h(z) = g(z)$ where $g(z)$ is a yet undefined step-funtion.\n",
      "* $z=w^Tx + b$ is a linear combination of features and parameters.\n",
      "* If $y^i=1$, we train to generate $w^Tx + b >> 0$ and our classifier picks 1.\n",
      "\n",
      "$\\textbf{Assumptions} -$\n",
      "\n",
      "$\\textbf{Likelihood estimate} -$   \n",
      "\n",
      "* We did not use a probabilistic framing for SVMs.\n",
      "\n",
      "$\\textbf{Strategies for training our model parameters} -$  \n",
      "\n",
      "(1) Use convex optimization:\n",
      "\n",
      "* Our objective is to maximize the geometric margin.\n",
      "* We set this up as an optimization problem and set it up in dual form.\n",
      "* $f(w) = \\|w\\|^{2}$\n",
      "* $g_i(w) = -y^i (w^T x^i + b) + 1 $ with $g_i(w) \\leq 0$ for the optimal margin (meaning all points are correctly classified).\n",
      "* $ L(w,b,\\alpha) = \\|w\\|^{2} - \\sum\\limits_{i = 1}^{k} \\alpha_i [y^i (w^T x^i + b) -1] $\n",
      "* In the dual formulation, we first minimize the Lagrangian with respect to $\\omega$: $ \\theta(\\alpha,\\beta) = min_{\\omega,b} L(\\omega,b,\\alpha) $\n",
      "* This leads to the optimal parameter value, $\\omega$.\n",
      "* We further use this to find the values of $\\alpha$: $ max_{\\alpha; \\alpha \\geq 0} W(\\alpha)$.\n",
      "\n",
      "$\\textbf{Implementing the model} -$\n",
      "\n",
      "* Now, if we want to make a prediction for a new point, x: \n",
      "* $ \\omega^T x + b = (\\sum_{i=1}^m \\alpha_i y^i x^i)^T x + b = \\sum_{i=1}^m \\alpha_i y^i <x^i x> + b $\n",
      "* Where $\\alpha_i > 0$ only for points closest to the decision boundary (the support vector)!\n",
      "* Further, it involoves an inner product, meaning that we can Kernelize it to make this computation efficient.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Convex optimization - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Convex optimization: \n",
      "    \n",
      "* Find a global solution for objective function within polynomial time.\n",
      "\n",
      "Convex sets:\n",
      "\n",
      "* Think of the set as a circle.\n",
      "* We draw a line between any two points.\n",
      "* If all points on the line are within the set, the set ($C$) is convex.\n",
      "* All points on a straight line that passes between $x_1,x_2$ are described by: $ x_3 = \\theta x_1 + (1-\\theta) x_2 \\in C $.\n",
      "* If this statement is true for any $x,y$, then the set is convex.\n",
      "* Examples include: All real numbers, non-negative numbers, norm ball ($\\| 1 \\| \\leq 1$). \n",
      "* Importaintly, positive semi-definite matricies are a convex set.\n",
      "\n",
      "Convex functions:\n",
      "\n",
      "* Convex funtion: $ f(\\theta x_1 + (1-\\theta) x_2) \\leq f(x_1) + (1-\\theta)f(x_2) $\n",
      "* Will always have a global optimum.\n",
      "\n",
      "First order condition of convexity: \n",
      "\n",
      "* The intution here is that we show that a function looks like a quadratic.\n",
      "* $f(x_2) \\geq f(x_1) +\\nabla f(x_1)^T (x_2-x_1)$\n",
      "* Simply, $x_2-x_1$ is the $x$ distance between points.\n",
      "* $\\nabla f(x_1)^T$ is the gradient of the function at $x_1$.\n",
      "* $\\nabla f(x_1)^T (x_2-x_1)$ gives an estimate for $f(x_2)$.\n",
      "* We simply show that the actual value of $f(x_2)$ is always greater than this estimate.\n",
      "\n",
      "Second order condition of convexity: \n",
      "\n",
      "* The second derivative is $\\geq 0$.\n",
      "* For example, proove that $y=x^2$ is convex: the second derivative is 2, which is $\\geq 0$.\n",
      "* Note that this can easily be extended to more complex functions: if working with a matrix, simply take the Hessian.\n",
      "* Moreover, if Hessian matrix is positive semi-definite, then the function is convex.\n",
      "\n",
      "The convex set and function come together to solve problems with a specific form:\n",
      "\n",
      "* Minimize a convex function, $f(x)$.\n",
      "* Subject to $x \\in C$.\n",
      "* Also subject to inequality ($g_i(x) \\leq 0$) and equality ($h_i=0$) constraints.\n",
      "* Where $f(x)$ and $g_i(x)$ are both convex differentiable.\n",
      "* Where $h_i(x)$ is affine, meaning that it is both convex and concave. \n",
      "\n",
      "Special cases:\n",
      "\n",
      "* Linear programming: Objective is affine, inequality condition is affine.\n",
      "* Quadratic programming: Objective is quadratic (e.g., least squares cost function)\n",
      "\n",
      "Use of quadratic programming for least squares minimization:\n",
      "\n",
      "* Typical problem will be constrained least squares.\n",
      "* We want to minimize our model error.\n",
      "* Objective is $\\frac{1}{2} \\| Ax - b \\|^2$\n",
      "* With constraints: (1) matrix format, (2) equality and / or inequality constraints."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Linear decision boundary - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Previously, we showed that logistic regression is a classification algorithm. \n",
      "\n",
      "Given $h_{\\theta}(x)$ and the fact that $y \\in \\{0,1\\}$, $h_{\\theta}(x)=0.5$ is the decision boundary.\n",
      "\n",
      "This is produced when $\\theta^Tx = \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 = 0$.\n",
      "\n",
      "In turn, we can define a line of values for $\\theta_{1,2}$ over which the decision is ambiguous.\n",
      "\n",
      "In this case, the decision boundary is linear:\n",
      "\n",
      "$x_2 = -\\frac{\\theta_1}{\\theta_2} x_1 + \\theta_3 $"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "%matplotlib inline\n",
      "\n",
      "# Value for theta\n",
      "theta=np.array([4,-4,5],dtype=float)\n",
      "\n",
      "# Decision boundary\n",
      "plt.subplot(2,2,2)\n",
      "x_min=0\n",
      "x_max=10\n",
      "x=np.arange(x_min, x_max, .1 )\n",
      "y=-(1/theta[2])*(theta[1]*x+theta[0])\n",
      "plt.plot(x,y,lw=2,color='black',alpha=0.75)\n",
      "plt.xlim([0,10])\n",
      "plt.ylim([0,10])\n",
      "plt.title('Decision boundary',fontsize=10)\n",
      "plt.xlabel('$x_1$',fontsize=10)\n",
      "plt.ylabel('$x_2$',fontsize=10)\n",
      "\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAOoAAACmCAYAAADUDhNZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1075b0390>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course, we train our model to obtain parameter values ($\\theta_{1,2,n}$) that seperate our data.\n",
      " \n",
      "Yet, in some cases, a linear decision boundary will not nicely seperate our data.\n",
      "\n",
      "Thus, we need to define a classification algorithm that implement decision boundaries with different shapes."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Functional and geometric margin - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assume our training set is linearly seperable.\n",
      "\n",
      "$g(z)$ is the logistic funtion where $z=\\theta^T x$:\n",
      "    \n",
      "* If $y^i=1$, we train to generate $\\theta^T x >> 0$ and our classifier picks 1.\n",
      "* If $y^i=0$, we train to generate $\\theta^T x << 0$ and our classifier picks 0.\n",
      "\n",
      "The concept of $\\textbf{funtional margin}$ simply tells us whether a given training example is properly classified.\n",
      "\n",
      "Given a training dataset that is linealy seperable, some bounaries come closer to training examples (less favorable) than others.\n",
      "\n",
      "Moreover, we might choose the decision bounary that stays far away from our training examples.\n",
      "\n",
      "The concept of $\\textbf{geometric margin}$ tells us how close the decision boundary is to each training example.\n",
      "\n",
      "Useful review:\n",
      "\n",
      "* http://stackoverflow.com/questions/20058036/svm-what-is-a-functional-margin"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The functional margin for the SVM classifier indicates whether training examples are properly classified -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For logistic regression:\n",
      "\n",
      "* Classes: $y \\in \\{0,1\\}$.\n",
      "* Hypothesis: $h(z) = g(z)$ where $g(z)$ is the logistic funtion.\n",
      "* Input: $z=\\theta^T x$ where a linear combination of features and parameters is used.\n",
      "* Funtional margin: if $y^i=1$, we train to generate $\\theta^T x >> 0$ and our classifier picks 1.\n",
      "\n",
      "For a new classifier that sets the stage for SVMs:\n",
      "\n",
      "* Classes: $y \\in \\{-1,1\\}$.\n",
      "* Hypothesis: $h(z) = g(z)$ where $g(z)$ is a yet undefined step-funtion.\n",
      "* It picks 1 if $z \\geq 0$ or it picks 0.\n",
      "* Input: $z=w^Tx + b$, again a linear combination of features and parameters is used.\n",
      "* Funtional margin: if $y^i=1$, we train to generate $w^Tx + b >> 0$ and our classifier picks 1.\n",
      "\n",
      "The funtional margin with respect to a single training example:\n",
      "\n",
      "$\\hat{\\gamma}_i = y^i (w^T x^i + b)$\n",
      "\n",
      "The funtional margin with respect to a full training set is defined as the worst-case functonal margin:\n",
      "\n",
      "$\\hat{\\gamma} = min (\\hat{\\gamma}^i)$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Decision boundary -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As with logistic regression, the decision boundary is produced when $\\omega^Tx + b = \\omega_1 x_1 + \\omega_2 x_2 + b = 0$.\n",
      "\n",
      "The linear decision boundary is:\n",
      "\n",
      "$x_2 = -\\frac{\\omega_1}{\\omega_2} x_1 - \\frac{b}{\\omega_2} $\n",
      "\n",
      "Note that the slope of the decision boundary is $-\\frac{\\omega_1}{\\omega_2}$, which is orthogomal to the slope defined by the parameters, $w$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Value for theta\n",
      "w=np.array([4,4],dtype=float)\n",
      "b=5\n",
      "\n",
      "# Vals\n",
      "x_min=-10\n",
      "x_max=10\n",
      "\n",
      "# Ensure equal figure dimentions to see Orthogonality\n",
      "fig=plt.figure(figsize=(5,5))\n",
      "\n",
      "# Decision boundary\n",
      "plt.subplot(2,2,2)\n",
      "x=np.arange(x_min, x_max, .1 )\n",
      "y=-w[0]/w[1]*x-b/w[1]\n",
      "plt.plot(x,y,lw=2,color='black',alpha=0.75)\n",
      "plt.xlim([x_min,x_max])\n",
      "plt.ylim([x_min,x_max])\n",
      "plt.title('$\\omega$ and the decision boundary',fontsize=10)\n",
      "plt.xlabel('$x_1$',fontsize=10)\n",
      "plt.ylabel('$x_2$',fontsize=10)\n",
      "\n",
      "y_o=w[1]/w[0]*x\n",
      "plt.plot(x,y_o,lw=2,color='red',alpha=0.75)\n",
      "\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADLCAYAAAArzNwwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFEQQqiAKIghVEAURhCr8PyAgP0ZsQAgTAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1095c9e10>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We know that $\\omega$ (red) is orthogonal to the decision boundary:\n",
      "\n",
      "* We define the geometric margin, $\\gamma_i$, as the distance between a point and the boundary.\n",
      "* We also introduce the norm of $\\omega$: $\\sqrt(\\sum \\omega_i^2)$.\n",
      "* A unit vector in the direction of $\\omega$: $\\frac{\\omega}{\\|\\omega\\|}$\n",
      "\n",
      "The geometric margin ($\\gamma_i$)for any point, $x^i$, is derived knowing that $x^i - \\gamma_i \\frac{\\omega}{\\|\\omega\\|}$ is on the decision boundary.\n",
      "\n",
      "Any point on the decision boundary evaluates to zero: \n",
      "\n",
      "$$ w^T (x^i - \\gamma_i \\frac{\\omega}{\\|\\omega\\|}) + b $$\n",
      "\n",
      "Moreover, you can solve for $\\gamma_i$:\n",
      "\n",
      "$$ \\gamma_i = y^i ((\\frac{\\omega}{\\|\\omega\\|})^T x^i + \\frac{b}{\\|\\omega\\|}) $$\n",
      "\n",
      "Recall that the functional margin is:\n",
      "\n",
      "$\\hat{\\gamma}_i = y^i (\\omega^T x^i + b)$\n",
      "\n",
      "Thus the two margins are equal for model parameters with $\\|w\\|=1$.\n",
      "\n",
      "$\\gamma_i = \\frac{\\hat{\\gamma_i}}{\\|w\\|}$\n",
      "\n",
      "And, finally, the geometric margin for a set of training data is the minimum computed for the training set:\n",
      "\n",
      "$\\gamma = min (\\gamma_i) $"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The geometric margin for the SVM classifier indicates how close training data are to the decision boundary -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuitively, we want to find parameters ($\\omega,b$) that maximize the geometric margin for a training set:\n",
      "\n",
      "$max (\\frac{\\hat{\\gamma}}{\\|\\omega\\|})$\n",
      "\n",
      "The challenge is that we have a non-convex objective.\n",
      "\n",
      "We do several things to simply this:\n",
      "\n",
      "* Enforce that the funtional margin, $\\hat{\\gamma} = 1 $.\n",
      "* Thus, the geometric margin is $\\gamma = \\frac{1}{\\|w\\|}$.\n",
      "* Observe that maximizing $\\frac{1}{\\|w\\|}$ is the same as minimizing $\\frac{1}{2}(\\|w\\|)^2$\n",
      "\n",
      "We now want to find $w$ and $b$ such that:\n",
      "\n",
      "$max (\\frac{1}{\\|w\\|})$ or $min_{\\gamma,w,b} (\\|w\\|^{2})$, which is a quadratic constraint ($w^Tw$).\n",
      "\n",
      "Solving this problem (e.g., using QP solvers) gives us the optimal margin classifier!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The Lagrangian is a useful tool for solving optimization problems - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a convex function that we want to minimize with inequality and equality constraints:\n",
      "\n",
      "* $min_x$ $f(x)$ \n",
      "* $g_i(w) \\leq 0$ \n",
      "* $h_i(w) = 0$\n",
      "\n",
      "We can solve it using the Lagrangian, which is passed x (the primal variable) as well as $\\alpha$ and $\\beta$ where $L$ is convex in x:\n",
      "\n",
      "$$ L(x,\\alpha,\\beta) = f(x) + \\sum\\limits_{i=1}^m \\alpha_i g_i(x) + \\sum\\limits_{i=1}^m \\beta_i h_i(x) $$\n",
      "\n",
      "Importaintly:\n",
      "\n",
      "* The first term captures the inequality constrains: $g_i(w) \\leq 0$\n",
      "* The second term captures the equality constraints: $h_i(w) = 0$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The primal problem -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall our objective is $min_x$ $f(x)$.\n",
      "\n",
      "In the primal problem, we first maximize the Lagrangian function over all possible values of $\\alpha$ and $\\beta$.\n",
      "\n",
      "$$ \\theta_p(x) = max_{\\alpha, \\beta, \\alpha \\geq 0} L(x,\\alpha,\\beta) = f(x) + max_{\\alpha, \\beta, \\alpha \\geq 0} [ ... ] $$\n",
      "\n",
      "This is subject to the constraint that $\\alpha_i \\geq 0$.\n",
      "\n",
      "The problem is primal feasible if the initial constraints are satisfied: \n",
      "\n",
      "* $h_i=0$ \n",
      "* $g_i \\leq 0$.\n",
      "\n",
      "This is because:\n",
      "\n",
      "* The second summation drops due to $h_i=0$.\n",
      "* The first summation drops: $\\alpha_i = 0$) will maximize the funtion by knocking out the negative terms ($g_i \\leq 0$).\n",
      "\n",
      "Therefore, for values of $x$ that are primal feasible, the objective is:\n",
      "\n",
      "* $ min_x \\theta_p(x) = f(x) $\n",
      "* $P^* = min_x \\theta_p(x)$ is the optimal value of the objective.\n",
      "\n",
      "If constraints are violated (e.g., $g_i \\geq 0$):\n",
      "\n",
      "* Then the summation blows up and $ \\theta_p(x) = \\infty $."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The dual problem -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, recall our objective is $min_x$ $f(x)$.\n",
      "\n",
      "Another way of solving this is the dual problem, which reverses the order of operations.\n",
      "\n",
      "Previously we:\n",
      "\n",
      "* We maximized the Lagrangian function over all possible values of $\\alpha$ and $\\beta$.\n",
      "* For all values of $x$ that were primal feasible, we then got a simple objective that we then minimized over $x$.\n",
      "\n",
      "For the dual problem, we:\n",
      "\n",
      "* We minimize the Lagrangian function over all possible values of $x$.\n",
      "* Then, we get an objective that we maximize over all possible values of $\\alpha$ and $\\beta$!\n",
      "\n",
      "Thus, the dual objective, which is dual feasible if $\\alpha_i \\geq 0$.\n",
      "\n",
      "$$ \\theta(\\alpha,\\beta) = min_{x} L(x,\\alpha,\\beta) $$\n",
      "\n",
      "$$ D^* = max_{\\alpha,\\beta} \\theta(\\alpha,\\beta)$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Connecting the dual and primal problems - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Key points connecting the primal and dual problems:\n",
      "\n",
      "* Weak duality: The optimal value for the dual problem, d^*,  is $\\leq$ than that of the primal problem p^*. \n",
      "* And strong duality: $d^* = p^*$\n",
      "\n",
      "Therefore:\n",
      "\n",
      "* The role of the Lagrangian is to connect the primal and dual problems.\n",
      "* There are a set of parameters that solve both problems."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Karush-Kuhn-Tucker (KKT) conditions are parameters that simultanously solve the primal and dual problem - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These set of parameters that solve both dual and primal framing of the problem:\n",
      "\n",
      "* There is a value $\\omega^*$ that is a solution to the primal problem \n",
      "* There are values $\\alpha^*, \\beta^*$ that are solutions to the dual problem.\n",
      "\n",
      "These parameters therefore satisfy the KKT conditions, the most importaint of which is the dual complementarity condition:\n",
      "\n",
      "* $\\alpha^* g_i(\\omega^*) = 0$\n",
      "* $\\alpha^* \\geq 0$\n",
      "\n",
      "This implies that $\\alpha^* > 0$ only when $g_i(\\omega^*) = 0$.\n",
      "\n",
      "In the context of SVMs, recall that $g_i$ will be:\n",
      "\n",
      "* $g_i(w) = -y^i (w^T x^i + b) + 1 $\n",
      "* $g_i = 0$ for points with a funtional margin of 1.\n",
      "* Points with a funtional margin of 1 are definitionally closest to the decision boundary."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Set up and solve the Dual problem to find optimal parameter values $w,\\alpha$ that maximize the geometric margin -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Previously, we defined a generalized Lagrangian:\n",
      "    \n",
      "$$ L(x,\\alpha,\\beta) = f(x) + \\sum\\limits_{i=1}^m \\alpha_i g_i(x) + \\sum\\limits_{i=1}^m \\beta_i h_i(x) $$\n",
      "\n",
      "We can use this to maximize the geometric margin for the optimal margin classifier:\n",
      "\n",
      "* $f(w) = \\|w\\|^{2}$\n",
      "* $g_i(w) = -y^i (w^T x^i + b) + 1 $\n",
      "\n",
      "Perform the substitution, noting that there are no equality ($h_i$) constraints in our problem:\n",
      "\n",
      "$$ L(w,b,\\alpha) = \\|w\\|^{2} - \\sum\\limits_{i = 1}^{k} \\alpha_i [y^i (w^T x^i + b) -1] $$\n",
      "\n",
      "We proceed to find the dual form of this problem, which has the classic form:\n",
      "\n",
      "$ \\theta(\\alpha,\\beta) = min_{x} L(x,\\alpha,\\beta) $\n",
      "\n",
      "We re-write with $x=\\omega+b$ and also note that $\\beta$ term is not present:\n",
      "\n",
      "$ \\theta(\\alpha,\\beta) = min_{\\omega,b} L(\\omega,b,\\alpha) $\n",
      "\n",
      "We can find the minimum by setting the derivative of $\\theta(\\alpha,\\beta)$ to zero:\n",
      "\n",
      "$$ \\nabla L(\\omega,b,\\alpha) = \\omega  - \\sum_{i=1}^m \\alpha_i y^i x^i = 0 $$\n",
      "\n",
      "We do this and substitue back into the inital Lagrangian to get:\n",
      "\n",
      "$$ min_w [ L(w,b,\\alpha) ] = \\sum \\alpha_i - 0.5 \\sum_i \\sum_j y^i y^j \\alpha_i \\alpha_j - <x^i , x^j > - b \\sum \\alpha_i y^i $$\n",
      "\n",
      "Critically, our problem meets conditions required for $p^* = d^*$ and the KKT conditions.\n",
      "\n",
      "This implies that $\\alpha^* > 0$ only when $g_i(\\omega^*) = 0$, the points close to our boundary.\n",
      "\n",
      "Moreover, we have re-framed the optimization problem:\n",
      "\n",
      "$$ max_{\\alpha; \\alpha \\geq 0} W(\\alpha) = \\sum \\alpha_i - 0.5 \\sum_i \\sum_j y^i y^j \\alpha_i \\alpha_j - <x^i , x^j > - b \\sum \\alpha_i y^i $$\n",
      "\n",
      "By solving the dual problem, we found: \n",
      "\n",
      "* A set of optimal $\\alpha_i$ values.\n",
      "* From KKT, we know that the $\\alpha_i>0$ only for points closest to the decision boundary, the support vector.\n",
      "* The optimal value of our model parameters: $ \\omega  = \\sum_{i=1}^m \\alpha_i y^i x^i $\n",
      "\n",
      "Now, if we want to make a prediction for a new point, x: \n",
      "\n",
      "$$ \\omega^T x + b = (\\sum_{i=1}^m \\alpha_i y^i x^i)^T x + b = \\sum_{i=1}^m \\alpha_i y^i <x^i x> + b $$\n",
      "\n",
      "Note two critical points:\n",
      "\n",
      "* This can be Kernelized, because only inner products between new point and the support vectors appear. \n",
      "* The only values of $\\alpha > 0$ are those for the support vectors."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Apply the Dual problem to a different objective - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall the general form of our constraints for the primal problem:\n",
      "    \n",
      "* $min_x$ $f(x)$ \n",
      "* $g_i(w) \\leq 0$ \n",
      "* $h_i(w) = 0$\n",
      "    \n",
      "We were given the constraint that the functional margin is:\n",
      "\n",
      "* $y^i (w^T x^i + b) \\geq 1 $\n",
      "\n",
      "We re-wrote this constraint to fit the primary problem:\n",
      "\n",
      "* $g_i(w) = -y^i (w^T x^i + b) + 1 $    \n",
      "* $g_i(w) \\leq 0$.\n",
      "\n",
      "We can use this to maximize the geometric margin for a new problem, written in primal form:\n",
      "\n",
      "* $min_{w}$ $f(w) = \\frac{1}{2} w^T w $.\n",
      "* Subject to: $w^T x^i \\geq 1$.\n",
      "\n",
      "For this new problem, we should similarly re-write the constraint:\n",
      "\n",
      "* $g_i(w) = 1-(w^T x^i)$\n",
      "* $g_i(w) \\leq 0$\n",
      "\n",
      "And we write the Lagrangian:\n",
      "\n",
      "$$ L(w,b,\\alpha) = \\frac{1}{2} w^T w + \\sum\\limits_{i = 1}^{k} \\alpha_i [1-(w^T x^i)] $$\n",
      "\n",
      "In the dual form of the problem, we first minimize the Lagrangian with respect to $w,b$.\n",
      "\n",
      "Since there is no $b$ parameter, we simply take the gradient of the Lagrangian with respect to $w$:\n",
      "\n",
      "$$ \\nabla_w L(w,b,\\alpha) = w + \\sum\\limits_{i=1}^k - a_i x_i $$\n",
      "\n",
      "We use this to find our optimal value of $w$:\n",
      "\n",
      "$$ w = \\sum\\limits_{i=1}^k a_i x_i $$\n",
      "\n",
      "Recall that our initial framing of the dual problem:\n",
      "\n",
      "$$ D^* = max_{\\alpha,\\beta,\\alpha \\geq 0} min_{x} L(w,\\alpha,\\beta) $$\n",
      "\n",
      "As shown above, we have done:\n",
      "\n",
      "$min_{x} L(w,\\alpha,\\beta)$\n",
      "\n",
      "This has given us the optimal value of $w$.\n",
      "\n",
      "We plug it back into the Lagrangian:\n",
      "\n",
      "$$ D^* = max_{\\alpha \\geq 0} L(w,b,\\alpha) = \\frac{1}{2} ( \\sum\\limits_{i=1}^k a_i x_i)^T ( \\sum\\limits_{i=1}^k a_i x_i) + \\sum\\limits_{i = 1}^{k} \\alpha_i [1-(( \\sum\\limits_{i=1}^k a_i x_i)^T x^i)] $$\n",
      "\n",
      "Distribute terms and simplify:\n",
      "\n",
      "$$ D^* = max_{\\alpha \\geq 0} L(w,b,\\alpha) = \\sum\\limits_{i = 1}^{k} \\alpha_i - \\frac{1}{2} \\sum\\limits_{i=1}^k \\sum\\limits_{j=1}^k a_i a_j (x_i)^T x^j $$\n",
      "\n",
      "Note that this can be Kernalized, because only inner products of the data appear!\n",
      "\n",
      "To evaluate a point, $z$, we simply evaluate $w^Tz$ with $w^T = (\\sum\\limits_{i=1}^k a_i x_i)^T $."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Inner products -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a distance between vectors (inner product). It is essential to have a positive inner product.\n",
      "\n",
      "Furthmore, you can think of the inner product as a measure of distance.\n",
      "\n",
      "* If two vectors are orthogonal, their inner product will be small.\n",
      "* Conversely, similar vectors will have a large inner product.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The inner product can be represented as a Kernel function - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have previously been discussing linear models, which lead to a linear decision boundary:\n",
      "\n",
      "As with logistic regression, the decision boundary is produced when $\\omega^Tx + b = \\omega_1 x_1 + \\omega_2 x_2 + b = 0$.\n",
      "\n",
      "The linear decision boundary is:\n",
      "\n",
      "$x_2 = -\\frac{\\omega_1}{\\omega_2} x_1 - \\frac{b}{\\omega_2} $\n",
      "\n",
      "However, it may be useful to learn in a higher dimentional space (e.g., with respect to $x^2$, etc).\n",
      "\n",
      "For this, the idea of a feature mapping, $\\phi$, is a way to re-cast the attributes in a higher dimentional space.\n",
      "\n",
      "$$ \\phi(x) = \\begin{bmatrix} x_i x_j \\\\   \\\\   \\end{bmatrix} $$\n",
      "\n",
      "If we want to learn in this higher dimensional space using our new funtion (above), we need to replace $x$ with $\\phi(x)$.\n",
      "\n",
      "In turn, we need to take the dot product of features, $\\phi(x)$.\n",
      "\n",
      "$$ \\phi(x)^T \\phi(z) = \\sum\\limits_i \\sum\\limits_j x_i x_j z_i z_j = \\sum\\limits_i x_i z_i \\sum\\limits_j x_j z_j = (x^T z)^2 $$\n",
      "\n",
      "In turn, this is the Kernel given the mapping specified above. \n",
      "\n",
      "Work a more complex example to further highlight the principle:\n",
      "\n",
      "$$ \\phi(x) = \\begin{bmatrix} x_i x_j \\\\ ...  \\\\ x_3 x_3 \\\\ \\sqrt{2c}x_i \\\\ ... \\\\ \\sqrt{2c}x_3 \\\\ c \\end{bmatrix} $$\n",
      "\n",
      "$$ \\phi(x)^T \\phi(z) = \\sum\\limits_i x_i z_i \\sum\\limits_j x_j z_j +  \\sum\\limits_i \\sqrt{2c}x_i \\sqrt{2c}z_i + c^2 = (x^T z)^2 + \\sqrt{2c} (x^T z) + c^2 $$\n",
      "\n",
      "Note:\n",
      "$$ A^2 + 2AB + B^2 = (A+B)^2 $$\n",
      "\n",
      "Thus:\n",
      "\n",
      "$$ K(\\phi(x),\\phi(z)) = \\phi(x)^T \\phi(z) = (x^T z + C)^2 $$\n",
      "\n",
      "There are more interesting mapping functions, which may lead to a Kernel definition.\n",
      "\n",
      "Intuitively, the Kernel mapping shows that the inner product in a higher dimentional space is equal:\n",
      "\n",
      "* To the inner product computer in the lower dimentional space with $x,z$.\n",
      "* This reduces the number of computations considerably.\n",
      "* Moreover, this is a measure of the similarly of two attribute vectors mapped by $\\phi$ into a higher dimentional space.\n",
      "\n",
      "So, a Kernel funtion, $K$, is intuitively a measure of distance."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Kernels - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If an algorithm inclues inner products $<x, z>$ between attribute vectors, then we can replace this with K(x,z) where K is a kernel. This allows the algorithm to work efficiently in the high dimensional feature space corresponding to K at low computational cost.\n",
      "\n",
      "Example:\n",
      "\n",
      "* Seperate two classes that are represented in a 2D space as concentric circles.\n",
      "* Define $\\phi$ as a mapping that will transform x,y into a (higher) dimentional space in which the classes are seperated."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}