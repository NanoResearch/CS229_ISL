{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Naive Bayes - "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Lance Martin"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Preface - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are notes I wrote for myself while studying for PhD qualifying exams at Stanford. \n",
      "\n",
      "I wanted to published them so that others could benefit.\n",
      "\n",
      "They draw from two sources:\n",
      "\n",
      "* CS229, an excellent machine learning class at Stanford taught by Andrew Ng.\n",
      "* An Introduction to Statistical Learning (ISL), an excellent text by G. James, D. Witten,  T. Hastie and R. Tibshirani (Springer, 2013).\n",
      "\n",
      "Some of the figures are taken from ISL with permission from the authors."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Quick cheatsheet - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Algorithm type} -$\n",
      "\n",
      "* Classification: It will compute a class label.\n",
      "* Generative: It will try to learn the features, given the label, $P(x \\mid y)$.\n",
      "\n",
      "$\\textbf{Model} -$\n",
      "\n",
      "* $P(X \\mid y) = \\prod\\limits_{i=1}^{m} p(x_i \\mid y_i)$\n",
      "* $\\phi_{i|y=1} = p(x_i = 1|y = 1)$\n",
      "\n",
      "$\\textbf{Assumptions} -$\n",
      "  \n",
      "* For GDA, we defined a multinomial Gaussian such that each combination of features had a unique probability assigned to it!\n",
      "* But, that doesn't scale well to cases of many features (e.g., text classification with a large dictionary).\n",
      "* Here, we assume the occournace of every feature is conditionally independent given $y$.\n",
      "* That is the probability of observing the words \"buy\" and \"Viagra\" in a spam e-mail are independent.\n",
      "\n",
      "$\\textbf{Likelihood estimate} -$   \n",
      "\n",
      "* $ L = \\prod\\limits_{i=1}^{m} p(x_i \\mid y_i)$\n",
      "\n",
      "$\\textbf{Strategies for training our model parameters} -$  \n",
      "\n",
      "(1) Analytical approaches to find model parameters:\n",
      "\n",
      "* In this case, took the derivative of the likelihood function with respect to each and set equal to zero.\n",
      "* This allowed us to derive formulas for the maximum likelihood estimates of each parameter (e.g., $\\phi_{i|y=1}$).\n",
      "\n",
      "$\\textbf{Implementing the model} -$\n",
      "\n",
      "* This is the same as GDA!\n",
      "* We train maximum likelihood estimates of each parameter on data for each class.\n",
      "* For test data, we then compute the posterior probability that it belongs to each class, using Bayes rule:\n",
      "* $ P(Y=1|X) = \\frac{P(X|y=1)P(y=1)}{P(X)} $\n",
      "* To assign the test example, $X$, we simply compute the posterior probability for each of the classes and pick the better one!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Generative algorithms - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall what we did with linear discriminant analysis:\n",
      "\n",
      "* We assumed Gaussian density funtion for our features with respect to each class.\n",
      "* With the training data, we then parameterized Gaussian densities (using likelihood maximization).\n",
      "* For each test data, we computed posterior probabilities for each class and simply chose the best one.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "An approach without the multinomial distribution - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But, we assumed a multinomial distribution to model the probability of observing different features combinations for each class.\n",
      "\n",
      "Consider problems like text classification in which the feature space (e.g., a large dictionary of words) is huge.\n",
      "\n",
      "Given a class label (e.g., spam), it would be challenging to model all combinations using the multinomial distribution.\n",
      "\n",
      "---\n",
      "\n",
      "Consider another way to do text classification.\n",
      "\n",
      "We convert an e-mail into a bit vector where each position specifies a word in a large (e.g., 10,000 word) dictionary.\n",
      "\n",
      "Before, the multinomial distribution modeled a unique probability associated with each feature $combination$!\n",
      "\n",
      "Now, we just assume (Naive Bayes) that each word, $x_i$, is conditionally independent, given y:\n",
      "\n",
      "$P(X \\mid y) = P(x_1,x_2,x_3 \\mid y) = P(x_1 \\mid y)$$P(x_2 \\mid y)$$P(x_3 \\mid y) = \\prod\\limits_{i=1}^{m} p(x_i \\mid y_i)$\n",
      "\n",
      "When m indexes over the dictionary, the probability of each word appearing is independent of seeing any other.\n",
      "\n",
      "Using Bayes rule, we then compute the class for any given e-mail:\n",
      "\n",
      "$$ P(Y=1\\mid X) = \\frac{P(X \\mid y=1)P(y=1)}{P(X)} $$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Learning strategy - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As before, we define a likelihood funtion:\n",
      "\n",
      "* $ L = \\prod\\limits_{i=1}^{m} p(x_i \\mid y_i)$\n",
      "\n",
      "As with GDA, we use analytical approaches to find model parameters:\n",
      "\n",
      "* In this case, took the derivative of the likelihood function with respect to each and set equal to zero.\n",
      "* This allowed us to derive formulas for the maximum likelihood estimates of each parameter (e.g., $\\phi_{i|y=1}$)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Implementation - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Furthermore, in CS229 weshowed that we can compute the joint likelihood, which is the product of predicted value for our class. \n",
      "\n",
      "We can take the derivitive, set it to zero (to find the maximum), and then find the expected parameters.\n",
      "\n",
      "$$ l() = \\sum_{i=1}^m log (P(x \\mid y)) p(y)$$\n",
      "\n",
      "This results in very intuitive estimates for the model parameters:\n",
      "\n",
      "For example $P(j \\mid y=0)$ is simply the ocourance of the given word divided by the total number of e-mails in the class $y=0$.\n",
      "\n",
      "For test data, we then compute the posterior probability that it belongs to each class, using Bayes rule, $ P(Y=1|X) = \\frac{P(X|y=1)P(y=1)}{P(X)} $.\n",
      "\n",
      "To assign the test example, $X$, we simply compute the posterior probability for each of the classes and pick the better one!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn as sk\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_20newsgroups\n",
      "news = fetch_20newsgroups(subset='all')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "No handlers could be found for logger \"sklearn.datasets.twenty_newsgroups\"\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# FINISH THESE NOTES # "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}